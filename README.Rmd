---
output: 
  github_document:
    toc: true
    toc_depth: 2
    includes:
      before_body: header.md
    pandoc_args: --webtex
always_allow_html: yes
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

## Installation

__rTPC__ can easily be downloaded from GitHub using the `remotes::install_github()`.

```{r install, eval = FALSE}
# install package from GitHub
remotes::install_github("padpadpadpad/rTPC")
```

We also need to install a bunch of other packages that will help with the analysis pipeline.

```{r install_other_packages, eval = FALSE}
# install other packages
install.packages('nls.multstart')
install.packages('purrr')
install.packages('dplyr')
install.packages('tidyr')
install.packages('ggplot2')
install.packages('broom')
install.packages('MuMIn')
```

Then we need to load in all the necessary packages.

```{r setup_analysis, message=FALSE, warning=FALSE}
# load in packages
library(purrr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(broom)
library(MuMIn)
library(rTPC)
library(nls.multstart)

# write function to label ggplot2 panels
label_facets_num <- function(string){
  len <- length(string)
  string = paste('(', 1:len, ') ', string, sep = '')
  return(string)
}

# write function to convert label text size to points
pts <- function(x){
  as.numeric(grid::convertX(grid::unit(x, 'points'), 'mm'))
}
```

## Before you start modelling

Before fitting any of these models, it is likely that some filtering and cleaning of the data needs to be done. Some things you should consider are:

- How many data points do you have for each curve? The more points the better for getting the best curve fits, but at the very least, there needs to be more points than unknown parameters. To fit all of the curves included here, that would mean having a minimum of 7 points per curve.
- Do you have negative rate values? If you have a curve that crosses the x axis (i.e. there are negative rates), it might be beneficial to only consider models that are capable of modelling negative values. A summary of the models previously used in the literature is in the process of being made.
- How is your data organised? If you have multiple curves, it makes sense to have your data in long format, where grouping variables, temperature and rate have their own columns, and different curves take up multiple rows, rather than a single row encompassing a single curve.

After you are happy that you understand your dataset and want to start modelling your TPCs, we can do so using __rTPC__, [__nls.multstart__](https://github.com/padpadpadpad/nls.multstart) and tools from the __tidyverse__.

## Fitting multiple models to a single curve

__rTPC__ makes it easy to fit multiple different models to the same data. To fit each curve, we will use __nls.multstart__, which allows for multiple starting parameters when fitting non-linear regressions to improve the robustness and reproducibility of model fitting. 

__rTPC__ comes with a set of TPCs of photosynthesis and respiration of the aquatic algae, _Chlorella vulgaris_, from Padfield _et al._ (2016). There are 60 curves overall.

```{r quicklook, fig.height = 5, fig.width = 9, fig.align='center', message=FALSE, warning=FALSE}
# load in data
data("Chlorella_TRC")

# change rate to be non-log transformed
d <- mutate(Chlorella_TRC, rate = exp(ln.rate))

# show the data
ggplot(d, aes(temp, rate, group = curve_id)) +
  facet_grid(flux ~growth.temp) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  theme_bw()
```

First, we shall fit each model to just a single curve using __nls.multstart::nlsmultstart()__. To do this, we take advantage of some packages from the __tidyverse__, specifically __purrr__, __dplyr__, and __tidyr__. In short, we specify the grouping variables of the dataset (less applicable for a single curve) and then add a column for each different model fit. This results in a list column, where the actual fit is stored in the dataframe. This allows us increased flexibility compared to other methods such as extracting the parameters from the model and storing them in a list.

__rTPC__ contains helper functions to aid in model fitting. First, __get_start_vals()__ estimates start values for the specified model. These start values are estimated from the data where possible, but otherwise represent averages of previously reported parameters from the literature. Second, __get_lower_lims()__ and __get_upper_lims()__ set wide lower and upper limits for the model specified. These are needed as some models have multiple solutions, which means that some estimated parameters are  meaningless even though they represent the best solution. These are currently not available for all models, but we aim to do so soon.

```{r fit_models, warning=FALSE}
# filter data for just a single curve
d_1 <- filter(d, curve_id == 1)

# run in purrr - going to be a huge long command this one
d_models <- group_by(d_1, curve_id, growth.temp, process, flux) %>%
  nest() %>%
  mutate(., lactin2 = map(data, ~nls_multstart(rate ~ lactin2_1995(temp = temp, p, c, tmax, delta_t),
                       data = .x,
                       iter = 500,
                       start_lower = c(p = 0, c = -2, tmax = 35, delta_t = 0),
                       start_upper = c(p = 3, c = 0, tmax = 55, delta_t = 15),
                       supp_errors = 'Y')),
            sharpeschoolhigh = map(data, ~nls_multstart(rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                                           start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                                           supp_errors = 'Y')),
            johnsonlewin = map(data, ~nls_multstart(rate ~ johnsonlewin_1946(temp = temp, r0, e, eh, topt),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(r0 = 1e9, e = 0, eh = 0, topt = 270),
                                           start_upper = c(r0 = 1e11, e = 2, eh = 10, topt = 330),
                                           supp_errors = 'Y')),
            thomas = map(data, ~nls_multstart(rate ~ thomas_2012(temp = temp, a, b, c, topt),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'thomas_2012') - 1,
                                           start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'thomas_2012') + 2,
                                           supp_errors = 'Y',
                                           lower = c(a= 0, b = -10, c = 0, topt = 0))),
            briere2 = map(data, ~nls_multstart(rate ~ briere2_1999(temp = temp, tmin, tmax, a, b),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(tmin = 0, tmax = 20, a = -10, b = -10),
                                           start_upper = c(tmin = 20, tmax = 50, a = 10, b = 10),
                                           supp_errors = 'Y',
                                           lower = c(tmin = -10, tmax = 20, a = -10, b = -10),
                                           upper = c(tmin = 20, tmax = 80, a = 10, b = 10))),
            spain = map(data, ~nls_multstart(rate ~ spain_1982(temp = temp, a, b, c, r0),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(a = -1, b = -1, c = -1, r0 = -1),
                                           start_upper = c(a = 1, b = 1, c = 1, r0 = 1),
                                           supp_errors = 'Y')),
            ratkowsky = map(data, ~nls_multstart(rate ~ ratkowsky_1983(temp = temp, tmin, tmax, a, b),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(tmin = 0, tmax = 20, a = -10, b = -10),
                                           start_upper = c(tmin = 20, tmax = 50, a = 10, b = 10),
                                           supp_errors = 'Y')),
            boatman = map(data, ~nls_multstart(rate ~ boatman_2017(temp = temp, rmax, tmin, tmax, a, b),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(rmax = 0, tmin = 0, tmax = 35, a = -1, b = -1),
                                           start_upper = c(rmax = 2, tmin = 10, tmax = 50, a = 1, b = 1),
                                           supp_errors = 'Y')),
            flinn = map(data, ~nls_multstart(rate ~ flinn_1991(temp = temp, a, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(a = 0, b = -2, c = -1),
                                           start_upper = c(a = 30, b = 2, c = 1),
                                           supp_errors = 'Y')),
            gaussian = map(data, ~nls_multstart(rate ~ gaussian_1987(temp = temp, rmax, topt, a),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(rmax = 0, topt = 20, a = 0),
                                           start_upper = c(rmax = 2, topt = 40, a = 30),
                                           supp_errors = 'Y')),
            oneill = map(data, ~nls_multstart(rate ~ oneill_1972(temp = temp, rmax, tmax, topt, a),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(rmax = 1, tmax = 30, topt = 20, a = 1),
                                           start_upper = c(rmax = 2, tmax = 50, topt = 40, a = 2),
                                           supp_errors = 'Y')),
            joehnk = map(data, ~nls_multstart(rate ~ joehnk_2008(temp = temp, rmax, topt, a, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(rmax = 0, topt = 20, a = 0, b = 1, c = 1),
                                           start_upper = c(rmax = 2, topt = 40, a = 30, b = 2, c = 2),
                                           supp_errors = 'Y',
                                           lower = c(rmax = 0, topt = 0, a = 0, b = 1, c = 1))),
            kamykowski = map(data, ~nls_multstart(rate ~ kamykowski_1985(temp = temp, tmin, tmax, a, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(tmin = 0, tmax = 10, a = -3, b = -1, c = -1),
                                           start_upper = c(tmin = 20, tmax = 50, a = 3, b = 1, c =1),
                                           supp_errors = 'Y')),
            quadratic = map(data, ~nls_multstart(rate ~ quadratic_2008(temp = temp, a, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(a = 0, b = -2, c = -1),
                                           start_upper = c(a = 30, b = 2, c = 1),
                                           supp_errors = 'Y')),
            hinshelwood = map(data, ~nls_multstart(rate ~ hinshelwood_1947(temp = temp, a, e, c, eh),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(a = 1e9, e = 5, c = 1e9, eh = 0),
                                           start_upper = c(a = 1e11, e = 20, c = 1e11, eh = 20),
                                           supp_errors = 'Y')),
            sharpeschoolfull = map(data, ~nls_multstart(rate ~ sharpeschoolfull_1981(temp = temp, r_tref, e, el, tl, eh, th, tref = 15),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') - 10,
                                           start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') + 10,
                                           supp_errors = 'Y')),
            sharpeschoollow = map(data, ~nls_multstart(rate ~ sharpeschoollow_1981(temp = temp, r_tref, e, el, tl, tref = 15),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(r_tref = 0.01, e = 0, el = 0, tl = 270),
                                           start_upper = c(r_tref = 2, e = 3, el = 10, tl = 330),
                                           supp_errors = 'Y')),
            weibull = map(data, ~nls_multstart(rate ~ weibull_1995(temp = temp, a, topt, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = c(a = 0, topt = 30, b = 100, c = 10),
                                           start_upper = c(a = 3, topt = 50, b = 200, c = 50),
                                           lower = c(a = 0, topt = 20, b = 0, c = 0),
                                           supp_errors = 'Y')),
            rezende = map(data, ~nls_multstart(rate ~ rezende_2019(temp = temp, a, q10, b, c),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'rezende_2019') *0.8,
                                           start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'rezende_2019') * 1.2,
                                           upper = get_upper_lims(.x$temp, .x$rate, model_name = 'rezende_2019'),
                                           lower = get_lower_lims(.x$temp, .x$rate, model_name = 'rezende_2019'),
                                           supp_errors = 'Y')))

```

The predictions of each model can be extracted using __broom::augment()__, after we stack all of our models into long format. To allow for a smooth curve, we produce a new temperature vector that has 100 points instead of our original data which just has 8. 

We can also grab all the parameters of the model using __broom::tidy()__. This is useful if using just a single model, but as parameter meanings do not transfer well across models, using this to compare models is limited.

Nevertheless, we can extract parameters and visualise our model fits.

```{r plot_predictions, fig.height = 8, fig.width = 12, fig.align='center', warning=FALSE, message=FALSE}
# stack models
d_stack <- gather(d_models, 'model', 'output', 6:ncol(d_models))

# preds
newdata <- tibble(temp = seq(min(d_1$temp), max(d_1$temp), length.out = 100))
d_preds <- d_stack %>%
  mutate(., preds = map(output, augment, newdata = newdata)) %>%
  unnest(preds)

# estimate parameters
params <- d_stack %>%
  mutate(., est = map(output, tidy)) %>%
  select(., -c(data, output)) %>%
  unnest(est)

# plot fit
ggplot(d_preds, aes(temp, rate)) +
  geom_point(aes(temp, rate), d_1) +
  geom_line(aes(temp, .fitted, col = model)) +
  facet_wrap(~model, labeller = labeller(model = label_facets_num)) +
  theme_bw(base_size = 16) +
  theme(legend.position = 'none',
        strip.text = element_text(hjust = 0),
        strip.background = element_blank()) +
  xlab('Temperature (ºC)') +
  ylab('rate') +
  geom_hline(aes(yintercept = 0), linetype = 2)

```

If a given model has not fit, they are not included in the panel plot.

### Estimating traits of the TPC

A common motivation for fitting a particular model is that a desired term (e.g. the optimum temperature $T_{opt}$) is specifically defined in the model. However, these parameters can also be derived from any given model fit, by estimating parameters from high resolution predictions of the model.

__rTPC__ comes with a bunch of helper functions to estimate common traits that are extracted from TPCs, such as optimum temperature, __get_topt()__, maximal rate, __get_rmax()__, and activation energy, __get_e()__. These functions can all be called at once using __est_params()__.

```{r use_est_params, warning=FALSE}
# estimate extra parameters
params_extra <- d_stack %>%
  mutate(., est = map(output, est_params)) %>%
  select(., -c(data, output)) %>%
  unnest(est) %>%
  ungroup() %>%
  select(c(model, rmax:ncol(.))) %>%
  mutate_if(is.numeric, function(x) round(x, 2))

# set column names for html table
col_names <- c('model', 'Rmax', 'Topt', 'CTmin', 'CTmax', 'E', 'Eh', 'Q10', 'thermal\nsafety\nmargin', 'tolerance\nrange', 'skewness')

# show the table of extra parameters
head(params_extra) %>%
  kableExtra::kable(., col.names = col_names, align = 'c') %>%
  kableExtra::kable_styling(font_size = 14, bootstrap_options = c("striped", "hover", "condensed"), position = 'center')

```

## Fitting multiple models to multiple curves

One benefit of this pipeline using __purrr::map()__ is its scalability to fitting single or multiple models to multiple thermal performance curves. To demonstrate this, we fitted a few select models (gaussian, the full Sharpe-Schoolfield model, the Sharpe-Schoolfield model for high temperature inactivation, the quadratic model, and the Lactin2 model) to the first 10 curves in the example dataset.

In doing this, we alter the original code for __nls_multstart()__ to include a progress bar. We calculate the total number of models to be fitted as: $n_{tot} = n_{models} \times n_{TPCs}$.

```{r scale_up, warning=FALSE, message=TRUE}
# filter 15 curves
d_10 <- filter(d, curve_id <= 10)

# when scaling up our code to fit hundreds of models, its nice to have a progress bar
nls_multstart_progress <- function(formula, data = parent.frame(), iter, start_lower, 
                                   start_upper, supp_errors = c("Y", "N"), convergence_count = 100, 
                                   control, modelweights, ...){
  if(!is.null(pb)){
    pb$tick()$print()
  }
  nls_multstart(formula = formula, data = data, iter = iter, start_lower = start_lower, 
                start_upper = start_upper, supp_errors = supp_errors, convergence_count = convergence_count, 
                control = control, modelweights = modelweights, ...)
}


# start progress bar and estimate time it will take
number_of_models <- 5
number_of_curves <- length(unique(d_10$curve_id))

# setup progress bar
pb <- progress_estimated(number_of_curves*number_of_models)

# run each model on each curve
d_models <- group_by(d_10, curve_id, growth.temp, process, flux) %>%
  nest() %>%
  mutate(., lactin2 = map(data, ~nls_multstart_progress(rate ~ lactin2_1995(temp = temp, p, c, tmax, delta_t),
                       data = .x,
                       iter = 500,
                       start_lower = c(p = 0, c = -2, tmax = 35, delta_t = 0),
                       start_upper = c(p = 3, c = 0, tmax = 55, delta_t = 15),
                       supp_errors = 'Y')),
            sharpeschoolhigh = map(data, ~nls_multstart_progress(rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
                       data = .x,
                       iter = 500,
                       start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 5,
                       start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 5,
                       supp_errors = 'Y')),
            gaussian = map(data, ~nls_multstart_progress(rate ~ gaussian_1987(temp = temp, rmax, topt, a),
                        data = .x,
                        iter = 500,
                        start_lower = c(rmax = 0, topt = 20, a = 0),
                        start_upper = c(rmax = 2, topt = 40, a = 30),
                        supp_errors = 'Y')),
            quadratic = map(data, ~nls_multstart_progress(rate ~ quadratic_2008(temp = temp, a, b, c),
                        data = .x,
                        iter = 500,
                        start_lower = c(a = 0, b = -2, c = -1),
                        start_upper = c(a = 30, b = 2, c = 1),
                        supp_errors = 'Y')),
            sharpeschoolfull = map(data, ~nls_multstart_progress(rate ~ sharpeschoolfull_1981(temp = temp, r_tref, e, el, tl, eh, th, tref = 15),
                        data = .x,
                        iter = 500,
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') - 5,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') + 5,
                        supp_errors = 'Y')))

```

We can easily then visualise the fit of each curve for each fit.

```{r scale_up_preds, warning=FALSE, fig.height = 5, fig.width = 9, fig.align='center'}
# stack models
d_stack <- gather(d_models, 'model', 'output', 6:ncol(d_models))

# preds
new_data <- tibble(temp = seq(min(d_1$temp), max(d_1$temp), length.out = 100))

# get preds
d_preds <- d_stack %>%
  mutate(., preds = map(output, augment, newdata = newdata)) %>%
  unnest(preds)

# plot preds
lines <- data.frame(val = 0, curve_id = 1:10)

ggplot() +
  geom_point(aes(temp, rate), d_10) +
  geom_line(aes(temp, .fitted, group = model), alpha = 0.4, d_preds) +
  facet_wrap(~ curve_id, ncol = 5, labeller = labeller(curve_id = label_facets_num)) +
  theme_bw(base_size = 16) +
  theme(legend.position = 'none',
        strip.text = element_text(hjust = 0),
        strip.background = element_blank()) +
  xlab('Temperature (ºC)') +
  ylab('rate') +
  geom_hline(aes(yintercept = val), linetype = 2, lines) +
  ylim(c(-0.5, 2.5))
```

And extract the parameters for each model for each curve. We can then plot these across each model for each TPC.

```{r est_params_many, warning=FALSE, fig.height = 5, fig.width = 10, fig.align='center'}
# estimate extra parameters
params_extra <- d_stack %>%
  mutate(., est = map(output, est_params)) %>%
  select(., -c(data, output)) %>%
  unnest(est)

params_extra <- gather(params_extra, 'term', 'estimate', rmax:ncol(params_extra))

# create jitter position with seeding
pos <- position_dodge(0.4)

ggplot(params_extra, aes(model, estimate, group = curve_id)) +
  geom_line(position = pos, alpha = 0.5) +
  geom_point(position = pos, shape = 21, fill = 'white') +
  facet_wrap(~ term, scales = 'free_y', ncol = 5, labeller = labeller(term = label_facets_num)) +
  theme_bw() +
  theme(legend.position = 'none',
        strip.text = element_text(hjust = 0),
        strip.background = element_blank(),
        axis.text.x = element_text(angle = 30, hjust = 1))
```

We can see, somewhat unsurprisingly, that there is as much variation between models as there is between curves.

## Model selection

It is very easy to incorporate model selection approaches into fitting these models to TPCs. __broom::glance()__ returns summary information on each fit, inluding some information criterion such as __AIC__ and __BIC__. An example of this is shown below.

```{r quicklook_glance}
d_stack %>%
  mutate(., info = map(output, glance)) %>%
  select(., curve_id, model, info) %>%
  unnest(info) %>%
  arrange(curve_id) %>%
  select(curve_id, model, sigma, isConv, logLik, AIC, BIC, deviance, df.residual) %>%
  head(5) %>%
  kableExtra::kable(align = 'c', digits = 2) %>%
  kableExtra::kable_styling(font_size = 14, bootstrap_options = c("striped", "hover", "condensed"), position = 'center')
  
```


However, TPCs are often data-poor, with not many more points than parameters being fitted. In such instances, __AICc__ is useful as it applies a correction for small sample sizes and is implemented using __MuMIn::AICc()__. 

When doing model selection across multiple curves, it is important to have defined and consistent rules at the beginning of the process. Here, I define the "best" model for the data overall as the model that has the lowest __AICc__ score for the most individual TPCs.

```{r model_selection}
# calculate AICc score and filter for the best model for each curve_id
table <- mutate(d_stack, aic = map_dbl(output, possibly(MuMIn::AICc, NA))) %>%
  filter(., !is.na(aic)) %>%
  group_by(curve_id) %>%
  top_n(1, desc(aic)) %>%
  group_by(model) %>%
  tally() %>%
  full_join(., tibble(model = unique(d_stack$model))) %>%
  mutate(., n = replace_na(n, 0)) %>%
  kableExtra::kable(align = 'c', digits = 2) %>%
  kableExtra::kable_styling(font_size = 14, bootstrap_options = c("striped", "hover", "condensed"), position = 'center')

table
```

So using this specific multi-model selection approach over multiple TPCs, in this instance I would favour using the gaussian model for the entire analysis.

## Model averaging

In turn, perhaps we want to use information criterion to implement model averaging techniques. Using __AICc__ scores, we can calculate model weighted predictions of best overall model fit. We average the predictions using the weight of every given model, but it is very easy to use a cut-off for model inclusion of $\triangle \! AICc \! \le \! 2$ from the lowest __AICc__ score if desired. 

```{r model_average,fig.height=5, fig.width = 10, message=FALSE, warning=FALSE}
# calculate AICc score and weight models
d_stack <- mutate(d_stack, aic = map_dbl(output, possibly(MuMIn::AICc, NA))) %>%
  filter(., !is.na(aic)) %>%
  group_by(curve_id) %>%
  # filter(., aic - min(aic) <= 2) %>%
  mutate(., weight = MuMIn::Weights(aic)) %>%
  ungroup()

# calculate average prediction
ave_preds <- merge(d_preds, select(d_stack, model, weight, curve_id), by = c('model', 'curve_id')) %>%
  mutate(., temp = round(temp, 2)) %>%
  group_by(temp, curve_id) %>%
  summarise(., ave_pred = sum(.fitted*weight)) %>%
  ungroup()

# plot these
lines <- data.frame(val = 0, curve_id = 1:10)
ggplot() +
  geom_point(aes(temp, rate), d_10) +
  geom_line(aes(temp, .fitted, group = model), alpha = 0.1, d_preds) +
  geom_line(aes(temp, ave_pred), ave_preds, size = 1) +
  theme_bw(base_size = 16) +
  theme(legend.position = 'none',
        strip.text = element_text(hjust = 0),
        strip.background = element_blank()) +
  facet_wrap(~ curve_id, ncol = 5, labeller = labeller(curve_id = label_facets_num)) +
  xlab('Temperature (ºC)') +
  ylab('rate') +
  ylim(c(-0.5, 2.5)) +
  geom_hline(aes(yintercept = val), linetype = 2, lines)
```

There will be eventually be a version of __est_params()__ that can work on a set of these predictions.

## Incorporating model weights

Methods of fitting TPCs in the literature often fit models to averaged rate values at each temperature. This fits a model on the mean rate value, but often the uncertainty around the mean value is ignored.

Instead, we can incorporate model uncertainty using the __weights__ argument within __nls_multstart()__. The optimal weights are $1/\sigma$, so can easily be included if you have any value of uncertainty for your rate values.

I will demonstrate how it works by averaging across the 3 biological replicates for a given curve, calculating the standard deviation, $\sigma$ for each mean rate value and fitting the Sharpe-Schoolfield model for high temperature inactivation to (a) only the average rate at each temperature and (b) the average rate and the standard deviation at each temperature.

```{r example_model_weights, fig.height=5, fig.width = 6, out.width = '50%', fig.align='center'}
d_ave <- filter(d, process == 'adaptation', growth.temp == 20, flux == 'photosynthesis') %>%
  group_by(temp, K) %>%
  summarise(., sd = sd(rate),
            ave_rate = mean(rate)) %>%
  ungroup()

# run both models
d_models <- nest(d_ave) %>%
  mutate(., mod = map(data, ~nls_multstart(ave_rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981') - 10,
                                           start_upper = get_start_vals(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981') + 10,
                                           lower = get_lower_lims(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981'),
                                           upper = get_upper_lims(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981'),
                                           supp_errors = 'Y')),
         mod_weights = map(data, ~nls_multstart(ave_rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
                                           data = .x,
                                           iter = 500,
                                           start_lower = get_start_vals(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981') - 10,
                                           start_upper = get_start_vals(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981') + 10,
                                           lower = get_lower_lims(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981'),
                                           upper = get_upper_lims(.x$temp, .x$ave_rate, model_name = 'sharpeschoolhigh_1981'),
                                           supp_errors = 'Y',
                                           # include weights here!
                                           modelweights = 1/sd)))

# calculate predictions
d_stack <- gather(d_models, 'model', 'output', starts_with('mod'))

# preds
newdata <- tibble(temp = seq(min(d_ave$temp), max(d_ave$temp), length.out = 100),
                  K = seq(min(d_ave$K), max(d_ave$K), length.out = 100))
d_preds <- d_stack %>%
  mutate(., preds = map(output, augment, newdata = newdata)) %>%
  unnest(preds)

# plot
ggplot() +  
  geom_line(aes(temp, .fitted, group = model, col = model), d_preds) +
  geom_linerange(aes(x = temp, ymin = ave_rate - sd, ymax = ave_rate + sd), d_ave) +
  geom_point(aes(temp, ave_rate), d_ave, size = 3, shape = 21, fill = 'white') +
  theme_bw(base_size = 16) +
  theme(legend.position = 'none',
        strip.text = element_text(hjust = 0),
        strip.background = element_blank()) +
  xlab('Temperature (ºC)') +
  ylab('rate') +
  geom_hline(aes(yintercept = 0), linetype = 2) +
  scale_color_manual(values = c('black', 'red'))

```

Here, the curve that incorporates model weights (in red) makes very little difference to the curve fit, but this will obviously depend on how much variation there is in your uncertainty across temperatures.

## Bootstrapping model fits

